{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework3_RNN",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "DxSDLpxVSNb6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 2: Use an Encoder-Decoder recurrent neural network to add 2 numers\n"
      ]
    },
    {
      "metadata": {
        "id": "vDC2hks3Sh1o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Inspiration from here: \n",
        "https://machinelearningmastery.com/learn-add-numbers-seq2seq-recurrent-neural-networks/"
      ]
    },
    {
      "metadata": {
        "id": "pRzjInRChgIY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e1cbdd7c-185e-4e9b-f7de-8ee897b27295"
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "from random import seed\n",
        "from random import randint\n",
        "from numpy import array\n",
        "from math import ceil\n",
        "from math import log10\n",
        "from math import sqrt\n",
        "from numpy import argmax\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import RepeatVector\n",
        "from sklearn.metrics import mean_squared_error\n",
        " "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "FhPtcDf-cQNm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WZ-QUiHbS8UN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encoding"
      ]
    },
    {
      "metadata": {
        "id": "jPKKhH0mSaao",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 1: Transform digits to sequence of characters\n"
      ]
    },
    {
      "metadata": {
        "id": "VVeQmN8hSva0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "alphabet = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ' ']"
      ]
    },
    {
      "metadata": {
        "id": "xVkXi-EFTDYG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Next: One hot encoding"
      ]
    },
    {
      "metadata": {
        "id": "gMiw0uALfJkF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "300773a9-7e7d-4aaf-8c4e-bae25a1edfaa"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def encode_numbers(a, b, is_train=False, max_lenght=3):\n",
        "  \n",
        "  X = np.zeros(((max_lenght+1)*2, 11))\n",
        "  Y = np.zeros((max_lenght+1, 11))\n",
        "\n",
        "  if is_train:\n",
        "    s = a+b\n",
        "    sum_int = a+b\n",
        "   \n",
        "  a = [int(i) for i in str(a)]\n",
        "  b = [int(i) for i in str(b)]\n",
        "  \n",
        "  a,b = pad_sequences([a,b],maxlen = max_lenght + 1, value=10)\n",
        "#   print(a,b)\n",
        "  input_nr = np.concatenate([a, b])\n",
        "  \n",
        "  \n",
        "  for j, char in enumerate(input_nr):\n",
        "      X[j, char] = 1\n",
        "      \n",
        "  if is_train:\n",
        "    s = [int(i) for i in str(s)]\n",
        "    a,b,s = pad_sequences([a,b,s],maxlen = max_lenght + 1,value=10)    \n",
        "#     print(s)\n",
        "    \n",
        "    for j, char in enumerate(s):\n",
        "       Y[j, char] = 1\n",
        "    return X, Y, sum_int\n",
        "    \n",
        "  return X\n",
        "\n",
        "nr1 = 4\n",
        "nr2 = 123\n",
        "           \n",
        "alphabet = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ' ']\n",
        "n_chars = len(alphabet)\n",
        "print(n_chars)\n",
        "\n",
        "X,Y,sum_int = encode_numbers(nr1, nr2, True)\n",
        "print(\"X:\", X)\n",
        "print(\"Y:\", Y)\n",
        "print(X.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11\n",
            "X: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "Y: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "(8, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HocpXUJC1aKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "81e38125-add0-4e16-e2c8-ca15385fcf14"
      },
      "cell_type": "code",
      "source": [
        "max_lenght = 3\n",
        "n_numbers = 10000\n",
        "\n",
        "X_input = np.zeros((n_numbers, 2)).astype(int)\n",
        "\n",
        "for i in range(n_numbers):\n",
        "  X_input[i,0] = np.random.randint(10**max_lenght)\n",
        "  X_input[i,1] = np.random.randint(10**max_lenght)\n",
        "\n",
        "print(X_input)\n",
        "  "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[173 488]\n",
            " [650 459]\n",
            " [343 713]\n",
            " ...\n",
            " [922 890]\n",
            " [615 969]\n",
            " [860 524]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cTawWDrP2c07",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "          \n",
        "alphabet = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ' ']\n",
        "n_chars = len(alphabet)\n",
        "\n",
        "XX_input = np.zeros((n_numbers, (max_lenght+1)*2, n_chars))\n",
        "YY = np.zeros((n_numbers, max_lenght+1, n_chars))\n",
        "\n",
        "for i in range(n_numbers):\n",
        "  XX_input[i], YY[i],sum_result = encode_numbers(X_input[i,0], X_input[i,1], True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LgvgCKAgTLP6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step2: Configure and Fit a seq2seq LSTM Model\n"
      ]
    },
    {
      "metadata": {
        "id": "C1MLC7O5TTOm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "12c492fa-1e49-4856-c250-8e90645e8347"
      },
      "cell_type": "code",
      "source": [
        "# define LSTM configuration\n",
        "# n_batch = 10\n",
        "# n_epoch = 30\n",
        "alphabet = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ' ']\n",
        "n_chars = len(alphabet)\n",
        "largest = 255\n",
        "\n",
        "n_numbers =2\n",
        "n_in_seq_length = n_numbers * ceil(log10(largest+1)) + n_numbers\n",
        "# n_in_seq_length = 3\n",
        "\n",
        "n_out_seq_length = ceil(log10(n_numbers * (largest+1))) + 1 \n",
        "# create LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, input_shape=(n_in_seq_length, n_chars)))\n",
        "# model.add(LSTM(100, input_shape=XX_input.shape))\n",
        "model.add(RepeatVector(n_out_seq_length))\n",
        "model.add(LSTM(50, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(n_chars, activation='softmax')))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_17 (LSTM)               (None, 100)               44800     \n",
            "_________________________________________________________________\n",
            "repeat_vector_9 (RepeatVecto (None, 4, 100)            0         \n",
            "_________________________________________________________________\n",
            "lstm_18 (LSTM)               (None, 4, 50)             30200     \n",
            "_________________________________________________________________\n",
            "time_distributed_9 (TimeDist (None, 4, 11)             561       \n",
            "=================================================================\n",
            "Total params: 75,561\n",
            "Trainable params: 75,561\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6LdChxsQTmkf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Train model:"
      ]
    },
    {
      "metadata": {
        "id": "_gUIre6G4Pm9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7235
        },
        "outputId": "8f9fb137-f519-4a19-fcad-950a59f06adc"
      },
      "cell_type": "code",
      "source": [
        "# \tmodel.fit(XX_input, YY, epochs=1, batch_size=n_batch)\n",
        "# \tmodel.fit(XX_input, YY, epochs=400)\n",
        "# \tmodel.fit(XX_input, YY, epochs=100)\n",
        "\n",
        "model.fit(XX_input, YY, epochs=200)\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 2.2175 - acc: 0.1983\n",
            "Epoch 2/200\n",
            "3000/3000 [==============================] - 2s 543us/step - loss: 1.8992 - acc: 0.2552\n",
            "Epoch 3/200\n",
            "3000/3000 [==============================] - 2s 532us/step - loss: 1.8667 - acc: 0.2679\n",
            "Epoch 4/200\n",
            "3000/3000 [==============================] - 2s 520us/step - loss: 1.8379 - acc: 0.2858\n",
            "Epoch 5/200\n",
            "3000/3000 [==============================] - 2s 541us/step - loss: 1.8223 - acc: 0.2886\n",
            "Epoch 6/200\n",
            "3000/3000 [==============================] - 2s 541us/step - loss: 1.8001 - acc: 0.3037\n",
            "Epoch 7/200\n",
            "3000/3000 [==============================] - 2s 539us/step - loss: 1.7633 - acc: 0.3224\n",
            "Epoch 8/200\n",
            "3000/3000 [==============================] - 2s 541us/step - loss: 1.7084 - acc: 0.3507\n",
            "Epoch 9/200\n",
            "3000/3000 [==============================] - 2s 533us/step - loss: 1.6635 - acc: 0.3641\n",
            "Epoch 10/200\n",
            "3000/3000 [==============================] - 2s 541us/step - loss: 1.6131 - acc: 0.3857\n",
            "Epoch 11/200\n",
            "3000/3000 [==============================] - 2s 533us/step - loss: 1.5744 - acc: 0.4052\n",
            "Epoch 12/200\n",
            "3000/3000 [==============================] - 2s 540us/step - loss: 1.5534 - acc: 0.4048\n",
            "Epoch 13/200\n",
            "3000/3000 [==============================] - 2s 529us/step - loss: 1.5265 - acc: 0.4171\n",
            "Epoch 14/200\n",
            "3000/3000 [==============================] - 2s 529us/step - loss: 1.4906 - acc: 0.4457\n",
            "Epoch 15/200\n",
            "3000/3000 [==============================] - 2s 527us/step - loss: 1.4945 - acc: 0.4250\n",
            "Epoch 16/200\n",
            "3000/3000 [==============================] - 2s 526us/step - loss: 1.4649 - acc: 0.4469\n",
            "Epoch 17/200\n",
            "3000/3000 [==============================] - 2s 536us/step - loss: 1.4454 - acc: 0.4643\n",
            "Epoch 18/200\n",
            "3000/3000 [==============================] - 2s 536us/step - loss: 1.4343 - acc: 0.4672\n",
            "Epoch 19/200\n",
            "3000/3000 [==============================] - 2s 532us/step - loss: 1.4622 - acc: 0.4355\n",
            "Epoch 20/200\n",
            "3000/3000 [==============================] - 2s 523us/step - loss: 1.4093 - acc: 0.4788\n",
            "Epoch 21/200\n",
            "3000/3000 [==============================] - 2s 518us/step - loss: 1.4040 - acc: 0.4730\n",
            "Epoch 22/200\n",
            "3000/3000 [==============================] - 2s 531us/step - loss: 1.3878 - acc: 0.4858\n",
            "Epoch 23/200\n",
            "3000/3000 [==============================] - 2s 533us/step - loss: 1.3965 - acc: 0.4723\n",
            "Epoch 24/200\n",
            "3000/3000 [==============================] - 2s 528us/step - loss: 1.3941 - acc: 0.4686\n",
            "Epoch 25/200\n",
            "3000/3000 [==============================] - 2s 534us/step - loss: 1.3651 - acc: 0.4938\n",
            "Epoch 26/200\n",
            "3000/3000 [==============================] - 2s 523us/step - loss: 1.3708 - acc: 0.4825\n",
            "Epoch 27/200\n",
            "3000/3000 [==============================] - 2s 533us/step - loss: 1.3554 - acc: 0.4955\n",
            "Epoch 28/200\n",
            "3000/3000 [==============================] - 2s 516us/step - loss: 1.3421 - acc: 0.5011\n",
            "Epoch 29/200\n",
            "3000/3000 [==============================] - 2s 516us/step - loss: 1.3340 - acc: 0.5067\n",
            "Epoch 30/200\n",
            "3000/3000 [==============================] - 2s 521us/step - loss: 1.3349 - acc: 0.4981\n",
            "Epoch 31/200\n",
            "3000/3000 [==============================] - 2s 532us/step - loss: 1.3250 - acc: 0.5055\n",
            "Epoch 32/200\n",
            "3000/3000 [==============================] - 2s 525us/step - loss: 1.3296 - acc: 0.4996\n",
            "Epoch 33/200\n",
            "3000/3000 [==============================] - 2s 523us/step - loss: 1.3145 - acc: 0.5073\n",
            "Epoch 34/200\n",
            "3000/3000 [==============================] - 2s 528us/step - loss: 1.3112 - acc: 0.5100\n",
            "Epoch 35/200\n",
            "3000/3000 [==============================] - 2s 515us/step - loss: 1.3039 - acc: 0.5156\n",
            "Epoch 36/200\n",
            "3000/3000 [==============================] - 2s 525us/step - loss: 1.3163 - acc: 0.5027\n",
            "Epoch 37/200\n",
            "3000/3000 [==============================] - 2s 534us/step - loss: 1.3035 - acc: 0.5080\n",
            "Epoch 38/200\n",
            "3000/3000 [==============================] - 2s 525us/step - loss: 1.2916 - acc: 0.5197\n",
            "Epoch 39/200\n",
            "3000/3000 [==============================] - 2s 523us/step - loss: 1.2910 - acc: 0.5129\n",
            "Epoch 40/200\n",
            "3000/3000 [==============================] - 2s 526us/step - loss: 1.2790 - acc: 0.5212\n",
            "Epoch 41/200\n",
            "3000/3000 [==============================] - 2s 521us/step - loss: 1.2756 - acc: 0.5269\n",
            "Epoch 42/200\n",
            "3000/3000 [==============================] - 2s 526us/step - loss: 1.2733 - acc: 0.5223\n",
            "Epoch 43/200\n",
            "3000/3000 [==============================] - 2s 518us/step - loss: 1.2730 - acc: 0.5245\n",
            "Epoch 44/200\n",
            "3000/3000 [==============================] - 2s 516us/step - loss: 1.2642 - acc: 0.5272\n",
            "Epoch 45/200\n",
            "3000/3000 [==============================] - 2s 512us/step - loss: 1.2674 - acc: 0.5265\n",
            "Epoch 46/200\n",
            "3000/3000 [==============================] - 2s 514us/step - loss: 1.2590 - acc: 0.5334\n",
            "Epoch 47/200\n",
            "3000/3000 [==============================] - 2s 517us/step - loss: 1.2566 - acc: 0.5293\n",
            "Epoch 48/200\n",
            "3000/3000 [==============================] - 2s 514us/step - loss: 1.2609 - acc: 0.5294\n",
            "Epoch 49/200\n",
            "3000/3000 [==============================] - 2s 525us/step - loss: 1.2524 - acc: 0.5298\n",
            "Epoch 50/200\n",
            "3000/3000 [==============================] - 2s 531us/step - loss: 1.2452 - acc: 0.5357\n",
            "Epoch 51/200\n",
            "3000/3000 [==============================] - 2s 521us/step - loss: 1.2619 - acc: 0.5222\n",
            "Epoch 52/200\n",
            "3000/3000 [==============================] - 2s 513us/step - loss: 1.2385 - acc: 0.5408\n",
            "Epoch 53/200\n",
            "3000/3000 [==============================] - 2s 521us/step - loss: 1.2311 - acc: 0.5434\n",
            "Epoch 54/200\n",
            "3000/3000 [==============================] - 2s 522us/step - loss: 1.2464 - acc: 0.5323\n",
            "Epoch 55/200\n",
            "3000/3000 [==============================] - 2s 518us/step - loss: 1.2555 - acc: 0.5277\n",
            "Epoch 56/200\n",
            "3000/3000 [==============================] - 2s 523us/step - loss: 1.2402 - acc: 0.5335\n",
            "Epoch 57/200\n",
            "3000/3000 [==============================] - 2s 518us/step - loss: 1.2298 - acc: 0.5390\n",
            "Epoch 58/200\n",
            "3000/3000 [==============================] - 2s 510us/step - loss: 1.2185 - acc: 0.5464\n",
            "Epoch 59/200\n",
            "3000/3000 [==============================] - 2s 511us/step - loss: 1.2319 - acc: 0.5397\n",
            "Epoch 60/200\n",
            "3000/3000 [==============================] - 2s 518us/step - loss: 1.2121 - acc: 0.5494\n",
            "Epoch 61/200\n",
            "3000/3000 [==============================] - 2s 523us/step - loss: 1.2034 - acc: 0.5522\n",
            "Epoch 62/200\n",
            "3000/3000 [==============================] - 2s 523us/step - loss: 1.2044 - acc: 0.5556\n",
            "Epoch 63/200\n",
            "3000/3000 [==============================] - 2s 528us/step - loss: 1.2421 - acc: 0.5337\n",
            "Epoch 64/200\n",
            "3000/3000 [==============================] - 2s 524us/step - loss: 1.2056 - acc: 0.5505\n",
            "Epoch 65/200\n",
            "3000/3000 [==============================] - 2s 528us/step - loss: 1.1937 - acc: 0.5592\n",
            "Epoch 66/200\n",
            "3000/3000 [==============================] - 2s 525us/step - loss: 1.1857 - acc: 0.5617\n",
            "Epoch 67/200\n",
            "3000/3000 [==============================] - 2s 532us/step - loss: 1.1871 - acc: 0.5591\n",
            "Epoch 68/200\n",
            "3000/3000 [==============================] - 2s 523us/step - loss: 1.1878 - acc: 0.5593\n",
            "Epoch 69/200\n",
            "3000/3000 [==============================] - 2s 528us/step - loss: 1.1927 - acc: 0.5558\n",
            "Epoch 70/200\n",
            "3000/3000 [==============================] - 2s 522us/step - loss: 1.1747 - acc: 0.5651\n",
            "Epoch 71/200\n",
            "3000/3000 [==============================] - 2s 524us/step - loss: 1.1918 - acc: 0.5569\n",
            "Epoch 72/200\n",
            "3000/3000 [==============================] - 2s 515us/step - loss: 1.1628 - acc: 0.5699\n",
            "Epoch 73/200\n",
            "3000/3000 [==============================] - 2s 527us/step - loss: 1.1758 - acc: 0.5655\n",
            "Epoch 74/200\n",
            "3000/3000 [==============================] - 2s 525us/step - loss: 1.1616 - acc: 0.5693\n",
            "Epoch 75/200\n",
            "3000/3000 [==============================] - 2s 521us/step - loss: 1.1614 - acc: 0.5700\n",
            "Epoch 76/200\n",
            "3000/3000 [==============================] - 2s 519us/step - loss: 1.1607 - acc: 0.5698\n",
            "Epoch 77/200\n",
            "3000/3000 [==============================] - 2s 523us/step - loss: 1.1534 - acc: 0.5775\n",
            "Epoch 78/200\n",
            "3000/3000 [==============================] - 2s 533us/step - loss: 1.1368 - acc: 0.5823\n",
            "Epoch 79/200\n",
            "3000/3000 [==============================] - 2s 522us/step - loss: 1.1367 - acc: 0.5819\n",
            "Epoch 80/200\n",
            "3000/3000 [==============================] - 2s 524us/step - loss: 1.1329 - acc: 0.5850\n",
            "Epoch 81/200\n",
            "3000/3000 [==============================] - 2s 528us/step - loss: 1.1247 - acc: 0.5864\n",
            "Epoch 82/200\n",
            "3000/3000 [==============================] - 2s 532us/step - loss: 1.1219 - acc: 0.5858\n",
            "Epoch 83/200\n",
            "3000/3000 [==============================] - 2s 527us/step - loss: 1.1185 - acc: 0.5870\n",
            "Epoch 84/200\n",
            "3000/3000 [==============================] - 2s 528us/step - loss: 1.1401 - acc: 0.5720\n",
            "Epoch 85/200\n",
            "3000/3000 [==============================] - 2s 525us/step - loss: 1.1032 - acc: 0.5918\n",
            "Epoch 86/200\n",
            "3000/3000 [==============================] - 2s 524us/step - loss: 1.0998 - acc: 0.5942\n",
            "Epoch 87/200\n",
            "3000/3000 [==============================] - 2s 528us/step - loss: 1.0954 - acc: 0.5941\n",
            "Epoch 88/200\n",
            "3000/3000 [==============================] - 2s 543us/step - loss: 1.0910 - acc: 0.5928\n",
            "Epoch 89/200\n",
            "3000/3000 [==============================] - 2s 515us/step - loss: 1.0939 - acc: 0.5902\n",
            "Epoch 90/200\n",
            "3000/3000 [==============================] - 2s 530us/step - loss: 1.0977 - acc: 0.5850\n",
            "Epoch 91/200\n",
            "3000/3000 [==============================] - 2s 529us/step - loss: 1.0783 - acc: 0.5982\n",
            "Epoch 92/200\n",
            "3000/3000 [==============================] - 2s 528us/step - loss: 1.0687 - acc: 0.6014\n",
            "Epoch 93/200\n",
            "3000/3000 [==============================] - 2s 528us/step - loss: 1.0602 - acc: 0.6054\n",
            "Epoch 94/200\n",
            "3000/3000 [==============================] - 2s 542us/step - loss: 1.0657 - acc: 0.6001\n",
            "Epoch 95/200\n",
            "3000/3000 [==============================] - 2s 534us/step - loss: 1.0430 - acc: 0.6107\n",
            "Epoch 96/200\n",
            "3000/3000 [==============================] - 2s 552us/step - loss: 1.0271 - acc: 0.6195\n",
            "Epoch 97/200\n",
            "3000/3000 [==============================] - 2s 565us/step - loss: 1.0228 - acc: 0.6198\n",
            "Epoch 98/200\n",
            "3000/3000 [==============================] - 2s 578us/step - loss: 1.0248 - acc: 0.6164\n",
            "Epoch 99/200\n",
            "3000/3000 [==============================] - 2s 570us/step - loss: 1.0126 - acc: 0.6209\n",
            "Epoch 100/200\n",
            "3000/3000 [==============================] - 2s 530us/step - loss: 0.9969 - acc: 0.6277\n",
            "Epoch 101/200\n",
            "3000/3000 [==============================] - 2s 532us/step - loss: 0.9949 - acc: 0.6239\n",
            "Epoch 102/200\n",
            "3000/3000 [==============================] - 2s 526us/step - loss: 0.9751 - acc: 0.6373\n",
            "Epoch 103/200\n",
            "3000/3000 [==============================] - 2s 527us/step - loss: 0.9606 - acc: 0.6383\n",
            "Epoch 104/200\n",
            "3000/3000 [==============================] - 2s 524us/step - loss: 0.9409 - acc: 0.6526\n",
            "Epoch 105/200\n",
            "3000/3000 [==============================] - 2s 532us/step - loss: 0.9404 - acc: 0.6481\n",
            "Epoch 106/200\n",
            "3000/3000 [==============================] - 2s 539us/step - loss: 0.9200 - acc: 0.6593\n",
            "Epoch 107/200\n",
            "3000/3000 [==============================] - 2s 523us/step - loss: 0.9195 - acc: 0.6550\n",
            "Epoch 108/200\n",
            "3000/3000 [==============================] - 2s 528us/step - loss: 0.8975 - acc: 0.6636\n",
            "Epoch 109/200\n",
            "3000/3000 [==============================] - 2s 522us/step - loss: 0.8847 - acc: 0.6678\n",
            "Epoch 110/200\n",
            "3000/3000 [==============================] - 2s 537us/step - loss: 0.8671 - acc: 0.6736\n",
            "Epoch 111/200\n",
            "3000/3000 [==============================] - 2s 522us/step - loss: 0.8536 - acc: 0.6812\n",
            "Epoch 112/200\n",
            "3000/3000 [==============================] - 2s 525us/step - loss: 0.8350 - acc: 0.6892\n",
            "Epoch 113/200\n",
            "3000/3000 [==============================] - 2s 532us/step - loss: 0.8088 - acc: 0.6988\n",
            "Epoch 114/200\n",
            "3000/3000 [==============================] - 2s 518us/step - loss: 0.8074 - acc: 0.6970\n",
            "Epoch 115/200\n",
            "3000/3000 [==============================] - 2s 515us/step - loss: 0.7887 - acc: 0.7054\n",
            "Epoch 116/200\n",
            "3000/3000 [==============================] - 2s 526us/step - loss: 0.7761 - acc: 0.7077\n",
            "Epoch 117/200\n",
            "3000/3000 [==============================] - 2s 518us/step - loss: 0.7491 - acc: 0.7184\n",
            "Epoch 118/200\n",
            "3000/3000 [==============================] - 2s 519us/step - loss: 0.7326 - acc: 0.7261\n",
            "Epoch 119/200\n",
            "3000/3000 [==============================] - 2s 522us/step - loss: 0.7151 - acc: 0.7333\n",
            "Epoch 120/200\n",
            "3000/3000 [==============================] - 2s 523us/step - loss: 0.7090 - acc: 0.7340\n",
            "Epoch 121/200\n",
            "3000/3000 [==============================] - 2s 523us/step - loss: 0.6923 - acc: 0.7366\n",
            "Epoch 122/200\n",
            "3000/3000 [==============================] - 2s 529us/step - loss: 0.6815 - acc: 0.7446\n",
            "Epoch 123/200\n",
            "3000/3000 [==============================] - 2s 536us/step - loss: 0.6680 - acc: 0.7487\n",
            "Epoch 124/200\n",
            "3000/3000 [==============================] - 2s 533us/step - loss: 0.6388 - acc: 0.7645\n",
            "Epoch 125/200\n",
            "3000/3000 [==============================] - 2s 531us/step - loss: 0.6311 - acc: 0.7628\n",
            "Epoch 126/200\n",
            "3000/3000 [==============================] - 2s 529us/step - loss: 0.6041 - acc: 0.7851\n",
            "Epoch 127/200\n",
            "3000/3000 [==============================] - 2s 526us/step - loss: 0.5841 - acc: 0.7929\n",
            "Epoch 128/200\n",
            "3000/3000 [==============================] - 2s 513us/step - loss: 0.5766 - acc: 0.7931\n",
            "Epoch 129/200\n",
            "3000/3000 [==============================] - 2s 523us/step - loss: 0.5629 - acc: 0.7995\n",
            "Epoch 130/200\n",
            "3000/3000 [==============================] - 2s 512us/step - loss: 0.5513 - acc: 0.8071\n",
            "Epoch 131/200\n",
            "3000/3000 [==============================] - 2s 527us/step - loss: 0.5310 - acc: 0.8163\n",
            "Epoch 132/200\n",
            "3000/3000 [==============================] - 2s 531us/step - loss: 0.5244 - acc: 0.8183\n",
            "Epoch 133/200\n",
            "3000/3000 [==============================] - 2s 516us/step - loss: 0.5108 - acc: 0.8250\n",
            "Epoch 134/200\n",
            "3000/3000 [==============================] - 2s 528us/step - loss: 0.4978 - acc: 0.8273\n",
            "Epoch 135/200\n",
            "3000/3000 [==============================] - 2s 515us/step - loss: 0.5175 - acc: 0.8169\n",
            "Epoch 136/200\n",
            "3000/3000 [==============================] - 2s 517us/step - loss: 0.4923 - acc: 0.8319\n",
            "Epoch 137/200\n",
            "3000/3000 [==============================] - 2s 522us/step - loss: 0.4563 - acc: 0.8493\n",
            "Epoch 138/200\n",
            "3000/3000 [==============================] - 2s 525us/step - loss: 0.4414 - acc: 0.8561\n",
            "Epoch 139/200\n",
            "3000/3000 [==============================] - 2s 534us/step - loss: 0.4285 - acc: 0.8639\n",
            "Epoch 140/200\n",
            "3000/3000 [==============================] - 2s 532us/step - loss: 0.4133 - acc: 0.8721\n",
            "Epoch 141/200\n",
            "3000/3000 [==============================] - 2s 533us/step - loss: 0.4071 - acc: 0.8764\n",
            "Epoch 142/200\n",
            "3000/3000 [==============================] - 2s 527us/step - loss: 0.3960 - acc: 0.8777\n",
            "Epoch 143/200\n",
            "3000/3000 [==============================] - 2s 521us/step - loss: 0.3841 - acc: 0.8872\n",
            "Epoch 144/200\n",
            "3000/3000 [==============================] - 2s 515us/step - loss: 0.3849 - acc: 0.8832\n",
            "Epoch 145/200\n",
            "3000/3000 [==============================] - 2s 534us/step - loss: 0.3883 - acc: 0.8837\n",
            "Epoch 146/200\n",
            "3000/3000 [==============================] - 2s 513us/step - loss: 0.3560 - acc: 0.8983\n",
            "Epoch 147/200\n",
            "3000/3000 [==============================] - 2s 522us/step - loss: 0.3541 - acc: 0.8973\n",
            "Epoch 148/200\n",
            "3000/3000 [==============================] - 2s 531us/step - loss: 0.3387 - acc: 0.9040\n",
            "Epoch 149/200\n",
            "3000/3000 [==============================] - 2s 526us/step - loss: 0.3259 - acc: 0.9124\n",
            "Epoch 150/200\n",
            "3000/3000 [==============================] - 2s 525us/step - loss: 0.3137 - acc: 0.9188\n",
            "Epoch 151/200\n",
            "3000/3000 [==============================] - 2s 526us/step - loss: 0.3092 - acc: 0.9216\n",
            "Epoch 152/200\n",
            "3000/3000 [==============================] - 2s 524us/step - loss: 0.2971 - acc: 0.9267\n",
            "Epoch 153/200\n",
            "3000/3000 [==============================] - 2s 521us/step - loss: 0.2989 - acc: 0.9252\n",
            "Epoch 154/200\n",
            "3000/3000 [==============================] - 2s 531us/step - loss: 0.3231 - acc: 0.9103\n",
            "Epoch 155/200\n",
            "3000/3000 [==============================] - 2s 525us/step - loss: 0.2944 - acc: 0.9255\n",
            "Epoch 156/200\n",
            "3000/3000 [==============================] - 2s 520us/step - loss: 0.2743 - acc: 0.9333\n",
            "Epoch 157/200\n",
            "3000/3000 [==============================] - 2s 530us/step - loss: 0.2768 - acc: 0.9336\n",
            "Epoch 158/200\n",
            "3000/3000 [==============================] - 2s 524us/step - loss: 0.2679 - acc: 0.9354\n",
            "Epoch 159/200\n",
            "3000/3000 [==============================] - 2s 513us/step - loss: 0.2469 - acc: 0.9476\n",
            "Epoch 160/200\n",
            "3000/3000 [==============================] - 2s 529us/step - loss: 0.2461 - acc: 0.9471\n",
            "Epoch 161/200\n",
            "3000/3000 [==============================] - 2s 524us/step - loss: 0.2533 - acc: 0.9376\n",
            "Epoch 162/200\n",
            "3000/3000 [==============================] - 2s 531us/step - loss: 0.2395 - acc: 0.9437\n",
            "Epoch 163/200\n",
            "3000/3000 [==============================] - 2s 528us/step - loss: 0.2224 - acc: 0.9563\n",
            "Epoch 164/200\n",
            "3000/3000 [==============================] - 2s 524us/step - loss: 0.2336 - acc: 0.9473\n",
            "Epoch 165/200\n",
            "3000/3000 [==============================] - 2s 538us/step - loss: 0.2193 - acc: 0.9533\n",
            "Epoch 166/200\n",
            "3000/3000 [==============================] - 2s 519us/step - loss: 0.2272 - acc: 0.9465\n",
            "Epoch 167/200\n",
            "3000/3000 [==============================] - 2s 520us/step - loss: 0.2269 - acc: 0.9468\n",
            "Epoch 168/200\n",
            "3000/3000 [==============================] - 2s 516us/step - loss: 0.1974 - acc: 0.9645\n",
            "Epoch 169/200\n",
            "3000/3000 [==============================] - 2s 518us/step - loss: 0.1890 - acc: 0.9678\n",
            "Epoch 170/200\n",
            "3000/3000 [==============================] - 2s 524us/step - loss: 0.1778 - acc: 0.9724\n",
            "Epoch 171/200\n",
            "3000/3000 [==============================] - 2s 519us/step - loss: 0.1876 - acc: 0.9656\n",
            "Epoch 172/200\n",
            "3000/3000 [==============================] - 2s 516us/step - loss: 0.1988 - acc: 0.9591\n",
            "Epoch 173/200\n",
            "3000/3000 [==============================] - 2s 517us/step - loss: 0.1988 - acc: 0.9573\n",
            "Epoch 174/200\n",
            "3000/3000 [==============================] - 2s 523us/step - loss: 0.1773 - acc: 0.9655\n",
            "Epoch 175/200\n",
            "3000/3000 [==============================] - 2s 525us/step - loss: 0.1700 - acc: 0.9688\n",
            "Epoch 176/200\n",
            "3000/3000 [==============================] - 2s 546us/step - loss: 0.1533 - acc: 0.9782\n",
            "Epoch 177/200\n",
            "3000/3000 [==============================] - 2s 568us/step - loss: 0.1456 - acc: 0.9807\n",
            "Epoch 178/200\n",
            "3000/3000 [==============================] - 2s 568us/step - loss: 0.1400 - acc: 0.9828\n",
            "Epoch 179/200\n",
            "3000/3000 [==============================] - 2s 567us/step - loss: 0.1371 - acc: 0.9843\n",
            "Epoch 180/200\n",
            "3000/3000 [==============================] - 2s 565us/step - loss: 0.1393 - acc: 0.9817\n",
            "Epoch 181/200\n",
            "3000/3000 [==============================] - 2s 571us/step - loss: 0.1371 - acc: 0.9807\n",
            "Epoch 182/200\n",
            "3000/3000 [==============================] - 2s 564us/step - loss: 0.1288 - acc: 0.9849\n",
            "Epoch 183/200\n",
            "3000/3000 [==============================] - 2s 533us/step - loss: 0.1235 - acc: 0.9860\n",
            "Epoch 184/200\n",
            "3000/3000 [==============================] - 2s 519us/step - loss: 0.1268 - acc: 0.9823\n",
            "Epoch 185/200\n",
            "3000/3000 [==============================] - 2s 527us/step - loss: 0.1287 - acc: 0.9800\n",
            "Epoch 186/200\n",
            "3000/3000 [==============================] - 2s 521us/step - loss: 0.1236 - acc: 0.9826\n",
            "Epoch 187/200\n",
            "3000/3000 [==============================] - 2s 525us/step - loss: 0.1215 - acc: 0.9820\n",
            "Epoch 188/200\n",
            "3000/3000 [==============================] - 2s 517us/step - loss: 0.1248 - acc: 0.9800\n",
            "Epoch 189/200\n",
            "3000/3000 [==============================] - 2s 517us/step - loss: 0.0986 - acc: 0.9927\n",
            "Epoch 190/200\n",
            "3000/3000 [==============================] - 2s 514us/step - loss: 0.0959 - acc: 0.9917\n",
            "Epoch 191/200\n",
            "3000/3000 [==============================] - 2s 533us/step - loss: 0.0931 - acc: 0.9929\n",
            "Epoch 192/200\n",
            "3000/3000 [==============================] - 2s 526us/step - loss: 0.0957 - acc: 0.9905\n",
            "Epoch 193/200\n",
            "3000/3000 [==============================] - 2s 526us/step - loss: 0.0907 - acc: 0.9918\n",
            "Epoch 194/200\n",
            "3000/3000 [==============================] - 2s 530us/step - loss: 0.1002 - acc: 0.9893\n",
            "Epoch 195/200\n",
            "3000/3000 [==============================] - 2s 534us/step - loss: 0.1654 - acc: 0.9588\n",
            "Epoch 196/200\n",
            "3000/3000 [==============================] - 2s 531us/step - loss: 0.1252 - acc: 0.9746\n",
            "Epoch 197/200\n",
            "3000/3000 [==============================] - 2s 537us/step - loss: 0.1009 - acc: 0.9837\n",
            "Epoch 198/200\n",
            "3000/3000 [==============================] - 2s 527us/step - loss: 0.0742 - acc: 0.9955\n",
            "Epoch 199/200\n",
            "3000/3000 [==============================] - 2s 530us/step - loss: 0.0680 - acc: 0.9977\n",
            "Epoch 200/200\n",
            "3000/3000 [==============================] - 2s 530us/step - loss: 0.0641 - acc: 0.9983\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdb93c77908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "metadata": {
        "id": "07jBhs7vTYc5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step3: Evaluate model"
      ]
    },
    {
      "metadata": {
        "id": "9HepGEKKFzut",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "4c7b5146-88ea-4a5c-deac-5fb8b0d1d57b"
      },
      "cell_type": "code",
      "source": [
        "# def invert(value, n_numbers, largest):\n",
        "#   print(value)\n",
        "#   return round(value * float(largest * n_numbers))\n",
        "def decode(value, n_numbers):\n",
        "  print(value)\n",
        "  return 1\n",
        "\n",
        "def one_hot_decode(encoded_seq):\n",
        "\treturn [argmax(vector) for vector in encoded_seq]\n",
        "\n",
        "def integer_combine(n_array):\n",
        "  result = 0\n",
        "  for num in n_array:\n",
        "    if(num!=10):\n",
        "      result = result * 10 + round(num)\n",
        "  return result\n",
        "\n",
        "# def decode2(value, n_numbers):\n",
        "#   print(value)\n",
        "#   return 1\n",
        "\n",
        "y = YY.astype(int)\n",
        "# print(y)\n",
        "\n",
        "n_numbers = 1000\n",
        "print(y.shape)\n",
        "result1 = [integer_combine(one_hot_decode(x)) for x in y]\n",
        "print(result1)\n",
        "# [decode(x, n_numbers) for x in y]\n",
        "\n",
        "\n",
        "# print([invert(x, n_numbers, largest) for x in y])\n",
        "# print(invert(x, n_numbers, largest) for x in y])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3000, 4, 11)\n",
            "[661, 1109, 1056, 134, 1556, 553, 93, 538, 831, 869, 878, 1302, 1262, 921, 659, 229, 887, 1445, 684, 318, 1240, 142, 652, 924, 376, 650, 1363, 1415, 592, 1461, 1053, 1663, 642, 536, 866, 1568, 887, 1399, 567, 935, 874, 303, 1302, 1485, 912, 1497, 1738, 239, 1153, 1544, 838, 1060, 234, 74, 517, 606, 312, 1107, 1351, 1421, 837, 761, 1507, 960, 906, 633, 1356, 1118, 838, 1361, 1100, 852, 1037, 1883, 1234, 1190, 306, 1224, 137, 415, 1354, 1078, 935, 1491, 535, 1521, 1432, 322, 1068, 452, 1486, 1336, 994, 1830, 464, 1228, 1761, 421, 1146, 1291, 711, 1053, 284, 759, 940, 410, 1192, 871, 1013, 996, 1675, 104, 288, 1885, 1706, 845, 90, 1109, 1364, 962, 866, 843, 1157, 1333, 1265, 1302, 939, 313, 985, 1031, 1468, 391, 921, 892, 1141, 463, 914, 1129, 1869, 1386, 879, 905, 979, 491, 980, 1144, 697, 1628, 244, 1366, 558, 1033, 1294, 1158, 976, 1611, 1683, 1515, 1603, 876, 895, 1787, 1545, 1054, 1411, 941, 1499, 1147, 1561, 525, 1154, 1208, 1790, 1241, 251, 1232, 977, 910, 1518, 995, 966, 1359, 623, 1190, 1062, 360, 876, 1589, 692, 1941, 102, 1618, 1300, 914, 1590, 1457, 1203, 1758, 642, 652, 1070, 415, 1225, 1072, 840, 1120, 1062, 770, 1077, 483, 599, 1085, 1134, 1327, 120, 1417, 1676, 1079, 678, 481, 252, 866, 1524, 714, 360, 1145, 1296, 1429, 976, 1136, 1473, 984, 709, 649, 1457, 1254, 1022, 640, 105, 1418, 1752, 1315, 763, 1041, 909, 181, 1065, 1075, 1002, 983, 1301, 660, 1041, 916, 1049, 1652, 440, 1088, 1213, 595, 826, 993, 643, 1030, 346, 1070, 1190, 629, 1465, 1371, 997, 327, 879, 1467, 1375, 1444, 396, 971, 674, 1378, 1023, 188, 829, 889, 742, 1167, 1254, 1092, 1630, 1462, 1479, 1206, 775, 561, 1011, 1166, 682, 1240, 762, 1257, 600, 1623, 885, 1071, 222, 979, 356, 544, 1792, 820, 691, 1350, 684, 1438, 806, 1054, 1375, 1027, 1354, 815, 506, 542, 1371, 1055, 1535, 1084, 1131, 518, 816, 706, 100, 1167, 1450, 1029, 864, 533, 1597, 1314, 584, 523, 1565, 723, 458, 1548, 1396, 1393, 550, 154, 1537, 780, 1205, 1277, 907, 826, 1140, 1387, 1466, 1116, 1150, 767, 1439, 319, 949, 1417, 1494, 1407, 719, 441, 1190, 539, 672, 601, 781, 1451, 844, 1045, 666, 683, 454, 901, 1512, 1117, 1362, 1204, 215, 1050, 1682, 1405, 1161, 918, 1367, 1652, 1891, 1151, 340, 1178, 831, 1150, 851, 791, 578, 1702, 1330, 916, 429, 857, 1199, 1372, 1141, 760, 852, 755, 1251, 1138, 875, 306, 878, 449, 1570, 1716, 1744, 277, 548, 684, 1710, 577, 780, 777, 1547, 1118, 894, 1557, 583, 743, 839, 722, 894, 894, 820, 887, 1274, 1293, 1173, 1360, 923, 540, 1348, 1667, 763, 824, 1306, 1465, 801, 1236, 1663, 1819, 773, 405, 415, 1204, 1375, 1408, 792, 684, 212, 586, 1565, 1173, 1085, 1384, 1425, 384, 1181, 1620, 1690, 1654, 1440, 694, 1350, 1512, 557, 1040, 406, 1335, 874, 88, 1843, 453, 1113, 1029, 584, 499, 1069, 1262, 1150, 1359, 918, 1309, 689, 882, 1167, 813, 357, 288, 1575, 1088, 1014, 189, 1346, 695, 1664, 1478, 514, 854, 848, 1473, 1764, 760, 651, 676, 1272, 494, 1690, 460, 727, 1043, 1407, 1132, 962, 680, 1743, 1167, 1764, 674, 1162, 797, 695, 677, 904, 659, 1272, 693, 849, 1604, 1721, 1436, 1289, 1000, 764, 591, 725, 456, 795, 556, 1310, 870, 690, 747, 695, 1320, 717, 988, 756, 1045, 1404, 1513, 929, 1537, 1004, 854, 799, 1541, 521, 642, 722, 1035, 1315, 1373, 704, 1398, 1802, 1420, 411, 1164, 1897, 888, 1225, 1378, 1106, 947, 1067, 603, 536, 1530, 895, 712, 1025, 161, 1641, 665, 1435, 1458, 746, 1154, 1035, 727, 820, 1078, 1697, 348, 1260, 766, 1036, 790, 968, 787, 286, 655, 435, 912, 881, 1007, 1145, 318, 553, 588, 925, 370, 312, 212, 1392, 714, 1165, 624, 765, 1048, 1311, 981, 1617, 1067, 1166, 1390, 1517, 1347, 458, 1382, 793, 1434, 910, 114, 540, 1406, 896, 823, 1201, 1154, 1192, 1429, 934, 1234, 896, 785, 442, 1235, 1341, 951, 1581, 950, 791, 702, 1130, 1571, 534, 1172, 1528, 303, 821, 991, 1068, 1481, 661, 61, 1392, 691, 1578, 1815, 1754, 1231, 1234, 627, 204, 1158, 201, 1317, 748, 1087, 1231, 1298, 610, 573, 1157, 784, 820, 518, 1087, 1184, 995, 686, 751, 583, 370, 1218, 337, 1283, 353, 1570, 1550, 712, 537, 940, 1138, 1387, 1039, 1144, 623, 43, 1196, 1130, 1576, 205, 1166, 53, 681, 933, 733, 1030, 1171, 1308, 831, 946, 1230, 554, 1850, 389, 232, 749, 1554, 804, 882, 1478, 455, 1361, 1028, 878, 1041, 314, 1255, 816, 102, 228, 857, 404, 1714, 1857, 1191, 1528, 1251, 1108, 740, 1324, 1540, 1515, 1217, 1201, 764, 1284, 1749, 1228, 1445, 1895, 1338, 488, 1367, 919, 650, 1042, 369, 1530, 1560, 809, 1659, 720, 997, 807, 524, 1167, 178, 1315, 1237, 548, 1481, 294, 316, 959, 1102, 808, 1023, 1107, 301, 1091, 1110, 739, 1048, 579, 1077, 1319, 390, 1075, 1083, 610, 316, 465, 1834, 1625, 1867, 281, 174, 890, 1027, 484, 840, 787, 563, 1176, 939, 1720, 629, 611, 1503, 891, 812, 725, 895, 571, 1525, 1219, 808, 967, 1074, 531, 884, 670, 858, 467, 695, 821, 1786, 927, 770, 997, 1125, 794, 1452, 519, 977, 338, 1540, 965, 621, 969, 955, 1123, 1180, 1947, 825, 221, 1252, 1321, 476, 643, 728, 830, 809, 1115, 832, 1203, 1183, 1616, 687, 1778, 1744, 1316, 1502, 799, 1418, 922, 1717, 574, 1473, 1014, 661, 1616, 1343, 1055, 1530, 795, 762, 1622, 1278, 198, 1148, 1181, 1270, 1531, 1233, 1323, 1660, 1044, 960, 829, 899, 443, 1607, 286, 881, 845, 528, 1164, 370, 1152, 1269, 1317, 234, 522, 737, 1114, 1559, 1774, 1021, 567, 900, 681, 1739, 836, 1672, 605, 1303, 1041, 1103, 1245, 1319, 992, 994, 872, 1628, 1099, 1451, 745, 996, 627, 733, 1264, 709, 660, 829, 891, 518, 1028, 855, 1007, 1485, 1360, 314, 439, 1445, 977, 1782, 955, 1469, 722, 1477, 1193, 845, 643, 815, 1540, 1569, 1790, 1045, 1554, 1622, 1117, 530, 1253, 966, 1602, 134, 773, 1099, 1357, 613, 944, 840, 732, 1262, 80, 1434, 833, 414, 814, 1448, 497, 275, 435, 765, 639, 526, 1336, 941, 187, 697, 731, 1008, 535, 1194, 1604, 928, 303, 343, 1313, 1016, 1018, 1563, 1251, 1238, 1487, 1795, 1666, 1181, 1716, 1270, 1221, 940, 1386, 897, 221, 1894, 221, 1672, 599, 1164, 933, 1003, 739, 1620, 1231, 207, 922, 1168, 1003, 1026, 725, 995, 791, 835, 310, 1057, 1459, 429, 896, 1632, 388, 361, 1115, 672, 1186, 839, 310, 238, 462, 1142, 1047, 1760, 744, 1835, 710, 526, 1134, 1233, 1292, 225, 1133, 1299, 1313, 1535, 1355, 305, 410, 1455, 1454, 1011, 696, 357, 386, 1231, 1064, 438, 600, 560, 1463, 762, 1129, 702, 1265, 1020, 784, 776, 559, 1428, 1552, 883, 1567, 932, 183, 968, 726, 929, 736, 1363, 849, 1202, 1009, 959, 935, 1139, 1187, 705, 1198, 1521, 1402, 350, 840, 1540, 994, 278, 616, 955, 949, 881, 1409, 1285, 139, 911, 1360, 1440, 1147, 764, 803, 692, 437, 309, 825, 897, 1160, 1028, 824, 991, 201, 774, 1573, 659, 1040, 1089, 881, 1084, 788, 1177, 284, 1055, 293, 1219, 517, 768, 906, 1422, 1067, 907, 667, 1553, 930, 1753, 333, 1354, 309, 1396, 1143, 808, 469, 258, 889, 987, 754, 772, 1101, 1659, 1272, 545, 1555, 1379, 1200, 1095, 999, 707, 1838, 1416, 527, 698, 876, 752, 1207, 458, 945, 981, 1440, 1031, 1043, 1273, 887, 599, 571, 1284, 1319, 1342, 1545, 1475, 1292, 543, 968, 669, 539, 587, 855, 1389, 1424, 1042, 422, 694, 948, 1269, 1443, 1654, 1816, 1197, 1041, 1255, 493, 424, 862, 571, 687, 1052, 872, 1641, 1823, 1411, 515, 1194, 1158, 344, 1212, 1180, 1494, 1812, 627, 401, 1098, 599, 1020, 113, 1098, 1580, 1717, 974, 489, 1117, 476, 739, 1487, 1632, 792, 1621, 809, 713, 339, 1222, 1055, 1017, 990, 900, 863, 1567, 1088, 1584, 1263, 968, 1811, 855, 475, 607, 1825, 510, 847, 878, 296, 985, 1068, 1165, 1474, 494, 542, 1474, 597, 144, 1305, 737, 1182, 788, 538, 317, 1160, 1894, 852, 1210, 340, 1334, 607, 1046, 636, 683, 552, 1053, 1439, 1683, 723, 116, 1302, 1867, 637, 542, 802, 1706, 1165, 1490, 1646, 473, 626, 1130, 546, 1358, 1069, 1040, 355, 1463, 1511, 1359, 810, 263, 1460, 1429, 1385, 636, 1094, 929, 780, 621, 464, 372, 330, 749, 503, 725, 1454, 494, 831, 1696, 1309, 479, 1725, 726, 980, 377, 1147, 545, 1162, 897, 1681, 428, 516, 687, 571, 1272, 1279, 783, 941, 1012, 923, 1240, 1115, 1828, 1217, 1814, 511, 1051, 1554, 1456, 719, 1194, 994, 518, 1447, 1467, 1067, 1070, 1820, 1335, 1335, 308, 348, 598, 664, 975, 1302, 1410, 543, 1203, 276, 415, 498, 116, 1476, 979, 592, 313, 1086, 799, 1318, 1473, 691, 1008, 1248, 820, 987, 906, 912, 1056, 1373, 982, 524, 693, 1306, 1102, 456, 507, 1544, 1331, 1350, 432, 1237, 1277, 1042, 783, 751, 1452, 1446, 1408, 964, 948, 394, 1217, 1904, 1421, 1419, 882, 1085, 1040, 556, 1625, 298, 585, 1405, 1419, 594, 1068, 303, 746, 988, 1162, 750, 1191, 892, 643, 1342, 1281, 804, 595, 163, 709, 689, 1395, 844, 975, 1178, 1456, 1141, 742, 461, 567, 302, 729, 1233, 1582, 235, 1857, 790, 243, 586, 612, 1803, 1118, 734, 1000, 473, 934, 986, 1359, 392, 1071, 1072, 1580, 1625, 914, 1469, 1720, 1506, 1208, 947, 938, 747, 510, 247, 1356, 692, 1576, 1136, 1177, 1552, 1002, 1048, 1186, 469, 1697, 1022, 1146, 858, 354, 1336, 800, 1539, 1385, 1675, 611, 1496, 1083, 1255, 1426, 886, 1523, 1269, 835, 261, 651, 775, 649, 839, 1352, 553, 1698, 1598, 1426, 976, 163, 221, 952, 1211, 1476, 906, 351, 526, 990, 282, 1499, 1168, 260, 813, 1555, 617, 658, 1967, 845, 911, 1442, 806, 921, 739, 907, 1237, 549, 815, 1085, 711, 1835, 915, 399, 937, 791, 1647, 935, 974, 1282, 1065, 1281, 1142, 1283, 741, 1598, 534, 983, 1665, 1526, 848, 828, 1240, 1474, 1139, 943, 1738, 1062, 1395, 1614, 767, 1307, 1222, 1623, 455, 1134, 1295, 1244, 549, 489, 1177, 1437, 942, 639, 673, 509, 403, 1072, 917, 431, 445, 1295, 1223, 972, 751, 1058, 1544, 1193, 820, 1245, 699, 1089, 757, 350, 1132, 1601, 575, 1216, 1114, 1857, 487, 780, 1021, 1644, 441, 762, 630, 1245, 1324, 1528, 201, 548, 1514, 261, 518, 758, 1284, 1142, 972, 781, 1517, 1439, 1276, 1226, 990, 893, 305, 1187, 722, 1009, 1390, 1005, 1790, 1250, 734, 429, 720, 1284, 1390, 798, 1203, 1533, 1348, 1193, 921, 939, 1016, 304, 613, 669, 1194, 1777, 1107, 770, 1101, 1309, 1307, 626, 770, 1505, 1704, 1775, 1862, 1331, 1032, 641, 826, 467, 1111, 1624, 804, 1376, 911, 517, 1629, 976, 248, 1081, 1299, 998, 851, 1946, 940, 741, 1207, 538, 942, 1085, 1174, 1096, 1373, 1791, 908, 1501, 1641, 275, 1426, 1914, 1353, 210, 759, 1108, 1261, 337, 1198, 945, 1821, 581, 1738, 1231, 760, 459, 1046, 1048, 706, 326, 775, 655, 1103, 1031, 905, 1436, 1201, 1154, 1733, 1483, 626, 977, 556, 287, 804, 1395, 1384, 700, 236, 1009, 421, 750, 1282, 779, 816, 1103, 1615, 744, 886, 551, 1321, 45, 195, 589, 1634, 1093, 1417, 1695, 944, 787, 410, 898, 166, 313, 936, 1182, 1016, 803, 1181, 384, 14, 1160, 1476, 561, 1023, 655, 1007, 495, 968, 956, 1770, 822, 1470, 440, 744, 987, 154, 982, 904, 1401, 1079, 1654, 631, 1126, 729, 484, 681, 562, 877, 1161, 1439, 1017, 1023, 1283, 1181, 918, 1356, 747, 423, 1272, 1609, 129, 1029, 258, 1233, 600, 1211, 939, 1369, 528, 1637, 568, 380, 1191, 695, 488, 1381, 1883, 615, 1103, 1208, 1524, 966, 680, 1301, 1241, 1209, 596, 354, 1273, 363, 1231, 1741, 781, 1549, 1584, 686, 525, 534, 1074, 1162, 1465, 813, 867, 1322, 1116, 308, 1236, 1160, 1019, 1217, 413, 982, 1345, 974, 663, 1484, 795, 562, 706, 1002, 848, 1148, 791, 456, 1342, 1089, 471, 207, 1280, 1025, 1160, 838, 1626, 1341, 1642, 1696, 1055, 1535, 568, 1079, 893, 1626, 974, 1472, 1015, 1122, 1257, 998, 971, 675, 1423, 1661, 952, 1522, 886, 1226, 782, 870, 254, 1201, 1148, 1179, 1966, 491, 1323, 1205, 1037, 583, 1146, 1090, 1035, 1376, 1289, 1281, 897, 1525, 440, 1084, 1009, 1365, 1475, 939, 1172, 1655, 980, 1658, 1195, 1243, 1175, 967, 500, 1150, 473, 1208, 630, 395, 1085, 723, 815, 1226, 984, 967, 1660, 1242, 920, 941, 932, 1752, 1034, 1170, 917, 1312, 454, 1004, 602, 1873, 696, 1356, 1682, 1340, 1725, 638, 1303, 911, 385, 1274, 664, 643, 1314, 410, 1013, 919, 590, 550, 981, 330, 844, 1513, 258, 1121, 448, 1494, 1296, 1163, 1295, 598, 1427, 732, 406, 1093, 1170, 553, 1356, 1014, 1217, 1689, 987, 963, 841, 1002, 1960, 668, 1429, 566, 907, 1060, 1510, 999, 846, 914, 601, 1173, 1171, 558, 681, 796, 752, 582, 1037, 710, 1271, 1435, 1654, 862, 1439, 1231, 809, 1106, 650, 1283, 1415, 1535, 543, 1343, 873, 422, 1069, 1207, 852, 762, 1217, 620, 428, 1402, 1357, 1195, 354, 293, 681, 456, 1491, 1301, 872, 1568, 1112, 312, 500, 1157, 1815, 1425, 1416, 599, 1025, 794, 1140, 998, 384, 687, 521, 1863, 1873, 1174, 1183, 1815, 743, 1366, 983, 517, 1350, 1317, 519, 1294, 1542, 1379, 479, 1504, 1494, 1063, 822, 1485, 1652, 872, 1078, 1334, 494, 1885, 698, 865, 1691, 1234, 1347, 1338, 772, 731, 922, 1478, 1013, 404, 305, 1358, 1422, 611, 424, 1163, 1111, 937, 751, 1816, 878, 1516, 102, 1570, 781, 885, 397, 1039, 820, 993, 1290, 1421, 355, 1651, 743, 1483, 1084, 840, 1652, 1574, 1221, 570, 259, 862, 786, 1532, 1432, 1235, 1322, 1233, 1068, 1173, 1327, 745, 1718, 898, 814, 782, 817, 1709, 923, 1472, 982, 1310, 429, 955, 1229, 686, 888, 940, 1107, 868, 1210, 675, 1307, 1012, 876, 947, 737, 194, 1075, 1325, 1054, 1220, 842, 368, 1306, 961, 407, 1420, 488, 1151, 1214, 723, 873, 1582, 1180, 1418, 1563, 912, 1106, 604, 1343, 1230, 117, 849, 1175, 873, 470, 1139, 717, 1108, 726, 1249, 448, 1332, 982, 1323, 389, 759, 1100, 654, 1001, 1443, 1826, 1897, 1324, 948, 1601, 611, 1764, 1391, 749, 581, 801, 1540, 591, 660, 174, 872, 1216, 881, 1114, 1013, 791, 443, 961, 951, 718, 978, 944, 1732, 922, 1145, 837, 1739, 982, 1753, 411, 619, 460, 1700, 1090, 1570, 1357, 260, 882, 520, 408, 1020, 1053, 530, 889, 541, 1264, 247, 1117, 836, 1120, 1168, 1447, 1184, 324, 565, 1343, 1352, 878, 1399, 987, 819, 1055, 870, 1764, 791, 94, 343, 1471, 938, 1189, 859, 1010, 1088, 450, 1088, 913, 756, 902, 1699, 738, 446, 1159, 911, 1928, 1189, 808, 1353, 295, 1313, 891, 908, 811, 322, 1535, 866, 1365, 1036, 866, 611, 982, 1199, 1768, 1307, 606, 887, 985, 1725, 191, 1654, 802, 1246, 804, 825, 1042, 1645, 931, 936, 1734, 413, 1785, 1289, 867, 904, 943, 933, 1119, 1543, 708, 1196, 1713, 782, 1791, 1123, 872, 1234, 1454, 770, 1263, 744, 828, 1590, 1245, 1345, 766, 1243, 869, 596, 1321, 972, 1360, 1179, 1494, 709, 675, 1634, 1113, 1139, 1349, 1081, 384, 1361, 1787, 510, 1684, 1252, 1418, 473, 1198, 1364, 895, 897, 1348, 1007, 1104, 1600, 737, 1278, 1475, 1629, 544, 1118, 522, 1363, 601, 551, 438, 939, 1338, 974, 432, 785, 1221, 739, 1015, 1804, 1110, 1000, 388, 918, 1170, 1219, 695, 1340, 367, 469, 341, 492, 1678, 1149, 1092, 581, 814, 1255, 667, 1515, 716, 1088, 1184, 530, 1691, 1263, 1452, 1240, 1558, 1022, 1680, 1132, 1303, 915, 367, 843, 597, 1371, 1790, 1432, 1433, 217, 1109, 1205, 391, 138, 1416, 1257, 296, 1240, 1626, 1736, 900, 554, 477, 879, 999, 1556, 1177, 1374, 1107, 1332, 1968, 1179, 823, 994, 882, 1016, 1530, 1049, 512, 1272, 1555, 1313, 874, 547, 292, 996, 893, 469, 1115, 1159, 1309, 487, 1167, 1116, 1106, 63, 1262, 1485, 1821, 706, 1617, 664, 904, 1639, 1529, 1270, 226, 776, 1047, 1741, 713, 312, 1493, 800, 785, 598, 678, 1559, 875, 1176, 1544, 896, 1005, 1130, 1638, 956, 409, 840, 588, 1206, 1157, 1191, 858, 624, 1170, 1003, 1431, 1363, 824, 953, 882, 962, 607, 502, 1560, 1150, 1202, 528, 1387, 1218, 293, 1152, 1059, 179, 1097, 440, 968, 1362, 195, 1584, 519, 338, 713, 1777, 1120, 959, 1082, 1156, 660, 445, 911, 386, 1127, 1154, 854, 1731, 1629, 1655, 744, 1226, 1309, 739, 957, 1250, 822, 978, 1706, 160, 1133, 945, 394, 1464, 1634, 1312, 458, 970, 646, 577, 1227, 892, 806, 1037, 913, 319, 1610, 1011, 603, 465, 1011, 594, 1420, 515, 716, 1002, 871, 957, 753, 412, 1590, 1234, 865, 845, 1405, 1643, 1292, 557, 1454, 983, 552, 1063, 1228, 954, 418, 904, 172, 1343, 1040, 1373, 585, 1078, 732, 611, 921, 1083, 1826, 412, 349, 1486, 1660, 859, 685, 1033, 285, 1080, 1279, 1105, 450, 808, 124, 730, 617, 355, 1431, 293, 425, 1239, 1180, 856, 1198, 862, 993, 1867, 426, 144, 1564, 1153, 561, 591, 350, 720, 730, 1088, 1555, 916, 1785, 928, 625, 302, 1289, 1102, 752, 1766, 1291, 515, 375, 1244, 1224, 407, 760, 1092, 1722, 1437, 575, 1321, 951, 1132, 1423, 1378, 1086, 1159, 1248, 1361, 1036, 1219, 1410, 1171, 677, 1301, 886, 1402, 1067, 1912, 883, 918, 1043, 1083, 461, 911, 1306, 688, 1028, 779, 1634, 1066, 1552, 1155, 968, 1169, 928, 762, 847, 1399, 704, 1487, 1304, 1328, 690, 892, 912, 1076, 663, 671, 617, 399, 1356, 1097, 896, 1258, 1706, 1078, 429, 1328, 1321, 216, 815, 1159, 599, 591, 1317, 1324, 887, 699, 1407, 176, 273, 651, 981, 751, 1273, 1193, 799, 1331, 1126, 1613, 800, 1455, 841, 596, 1020, 984, 603, 1129, 745, 1100, 1592, 533, 1332, 1098, 1957, 982, 633, 585, 1071, 664, 146, 1087, 1105, 1262, 611, 591, 1083, 623, 1031, 1459, 1035, 1030, 1551, 1308, 678, 922, 1149, 1265, 1037, 415, 1608, 792, 1491, 1075, 595, 274, 1304, 741, 918, 1446, 789, 1000, 1642, 774, 583, 402, 1063, 862, 889, 680, 1272, 1680, 1377, 640, 1478, 1426, 1398, 370, 772, 661, 1000, 859, 427, 1466, 1185, 1101, 1114, 1613, 1425, 1007, 799, 929, 1293, 1166, 428, 836, 1814, 1439, 624, 824, 245, 1164, 870, 761, 409, 1555, 280, 1068, 550, 1700, 605, 1284, 548, 880, 707, 1370]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PT-LtxmnTcOq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1313
        },
        "outputId": "95547d55-bc33-4cfd-eaad-56b5d7a115ac"
      },
      "cell_type": "code",
      "source": [
        "from random import seed\n",
        "from random import randint\n",
        "from numpy import array\n",
        "from math import ceil\n",
        "from math import log10\n",
        "from math import sqrt\n",
        "from numpy import argmax\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import RepeatVector\n",
        "\n",
        "\n",
        "n_numbers = 3000\n",
        "\n",
        "# evaluate on some new patterns\n",
        "          \n",
        "alphabet = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ' ']\n",
        "n_chars = len(alphabet)\n",
        "\n",
        "XX_input = np.zeros((n_numbers, (max_lenght+1)*2, 11))\n",
        "YY = np.zeros((n_numbers, max_lenght+1, 11))\n",
        "print(YY)\n",
        "\n",
        "for i in range(n_numbers):\n",
        "  XX_input[i], YY[i], sum_result = encode_numbers(X_input[i,0], X_input[i,1], True)\n",
        "\n",
        "  \n",
        "# X, y = generate_data(n_samples, n_numbers, largest, alphabet)\n",
        "# result = model.predict(X, batch_size=n_batch, verbose=0)\n",
        "result = model.predict(XX_input)\n",
        "\n",
        "# print(XX_input[0])\n",
        "\n",
        "# print(result[0])\n",
        "# print(integer_combine(one_hot_decode(result[0])))\n",
        "# print(YY[0])\n",
        "# print(integer_combine(one_hot_decode(YY[0])))\n",
        "\n",
        "\n",
        "# calculate error\n",
        "# expected = YY\n",
        "# predicted = [invert(x, alphabet) for x in result]\n",
        "predicted = [integer_combine(one_hot_decode(x)) for x in result]\n",
        "expected = [integer_combine(one_hot_decode(x)) for x in YY]\n",
        "\n",
        "\n",
        "\n",
        "rmse = sqrt(mean_squared_error(expected, predicted))\n",
        "print('RMSE: %f' % rmse)\n",
        "\n",
        "# # show some examples\n",
        "for i in range(20):\n",
        "\tprint('Expected=%s, Predicted=%s' % (expected[i], predicted[i]))\n",
        "\tprint('Difference=%s' % (abs(expected[i] - predicted[i])))\n",
        "  "
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "RMSE: 2.569176\n",
            "Expected=661, Predicted=661\n",
            "Difference=0\n",
            "Expected=1109, Predicted=1109\n",
            "Difference=0\n",
            "Expected=1056, Predicted=1056\n",
            "Difference=0\n",
            "Expected=134, Predicted=134\n",
            "Difference=0\n",
            "Expected=1556, Predicted=1556\n",
            "Difference=0\n",
            "Expected=553, Predicted=553\n",
            "Difference=0\n",
            "Expected=93, Predicted=93\n",
            "Difference=0\n",
            "Expected=538, Predicted=538\n",
            "Difference=0\n",
            "Expected=831, Predicted=831\n",
            "Difference=0\n",
            "Expected=869, Predicted=869\n",
            "Difference=0\n",
            "Expected=878, Predicted=878\n",
            "Difference=0\n",
            "Expected=1302, Predicted=1392\n",
            "Difference=90\n",
            "Expected=1262, Predicted=1262\n",
            "Difference=0\n",
            "Expected=921, Predicted=921\n",
            "Difference=0\n",
            "Expected=659, Predicted=659\n",
            "Difference=0\n",
            "Expected=229, Predicted=229\n",
            "Difference=0\n",
            "Expected=887, Predicted=887\n",
            "Difference=0\n",
            "Expected=1445, Predicted=1445\n",
            "Difference=0\n",
            "Expected=684, Predicted=684\n",
            "Difference=0\n",
            "Expected=318, Predicted=318\n",
            "Difference=0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4rlT3qfsjkVv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: test????\n",
        "# loss, acc = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HkxLUIYeizuP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Save (and load) model\n"
      ]
    },
    {
      "metadata": {
        "id": "XYvnpqlLj5Pe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model.save('RNN_sum_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
        "\n",
        "# Forloading model:\n",
        "# model = load_model('RNN_sum_model.h5')\n",
        "# model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}